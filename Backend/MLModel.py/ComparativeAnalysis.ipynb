{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import cross_val_score,StratifiedKFold\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "#import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dir_path = os.getcwd()\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "dataFrame = pd.read_csv('../../AIBetic2Dataset/balanced_diabetes_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Showing head of the dataset\n",
    "dataFrame.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Showing null values \n",
    "dataFrame.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Showing the type of the dataset\n",
    "dataFrame.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Different Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a count plot for the 'class' column of the DataFrame\n",
    "class_distribution_plot = sns.countplot(x='class', hue='class', data=dataFrame, palette='viridis', legend=False)\n",
    "\n",
    "# Display the plot\n",
    "class_distribution_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a new figure and axes for the subplots\n",
    "figure, subplot_axes = plt.subplots(1, 2, figsize=(14,7))\n",
    "\n",
    "# Compute the counts of each class\n",
    "class_counts = dataFrame['class'].value_counts()\n",
    "\n",
    "# Define the color palette and explode parameters\n",
    "color_palette_pie = sns.color_palette(\"pastel\", 7)\n",
    "explode_params = [0.1, 0]\n",
    "\n",
    "# Create a pie chart on the first subplot with the new color palette\n",
    "subplot_axes[0].pie(class_counts, autopct='%1.0f%%', startangle=60, labels=[\"Positive\",\"Negative\"], colors=color_palette_pie, explode=explode_params, shadow=True, wedgeprops={\"linewidth\":2,\"edgecolor\":\"k\"})\n",
    "subplot_axes[0].set_title(\"Target Variable Composition\")\n",
    "\n",
    "# Create a bar plot on the second subplot with a different color\n",
    "class_counts.plot(kind='barh', ax=subplot_axes[1], color='skyblue',legend=False)\n",
    "for idx, val in enumerate(class_counts.values):\n",
    "    subplot_axes[1].text(val * 0.7, idx, str(val), weight='bold', fontsize=20)\n",
    "subplot_axes[1].set_title(\"Number of Instances for Each Class\")\n",
    "\n",
    "# Adjust the layout and display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Visualising the distribution of Gender\n",
    "gender_plot = sns.countplot(x=dataFrame['Gender'], hue=dataFrame['class'], palette='Greens')\n",
    "\n",
    "# Defining the criteria for the cross-tabulation\n",
    "cross_tab_criteria = ['Gender', 'class']\n",
    "\n",
    "# Creating a color map\n",
    "color_map = sns.light_palette(\"blue\", as_cmap=True)\n",
    "\n",
    "# Creating a cross-tabulation and normalizing the values by column\n",
    "cross_tab = pd.crosstab(dataFrame[cross_tab_criteria[0]], dataFrame[cross_tab_criteria[1]], normalize='columns')\n",
    "\n",
    "# Converting the proportions to percentages and rounding to 2 decimal places\n",
    "cross_tab_percentages = cross_tab.mul(100).round(2)\n",
    "\n",
    "# Applying a gradient coloring to the table\n",
    "styled_cross_tab = cross_tab_percentages.style.background_gradient(cmap=color_map)\n",
    "\n",
    "styled_cross_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis based on Gender\n",
    "dataFrame.groupby(\"Gender\")[\"class\"].value_counts().unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Females in the dataset turned out to be more more positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the distribution of Polyuria\n",
    "polyuria_plot = sns.countplot(x=dataFrame['Polyuria'], hue=dataFrame['class'], palette='Greens')\n",
    "\n",
    "# Defining the criteria for the cross-tabulation\n",
    "cross_tab_criteria = ['Polyuria', 'class']\n",
    "\n",
    "# Creating a color map\n",
    "color_map = sns.light_palette(\"blue\", as_cmap=True)\n",
    "\n",
    "# Creating a cross-tabulation and normalizing the values by column\n",
    "cross_tab = pd.crosstab(dataFrame[cross_tab_criteria[0]], dataFrame[cross_tab_criteria[1]], normalize='columns')\n",
    "\n",
    "# Converting the proportions to percentages and rounding to 2 decimal places\n",
    "cross_tab_percentages = cross_tab.mul(100).round(2)\n",
    "\n",
    "# Applying a gradient coloring to the table\n",
    "styled_cross_tab = cross_tab_percentages.style.background_gradient(cmap=color_map)\n",
    "\n",
    "styled_cross_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis based on Polyuria\n",
    "dataFrame.groupby(\"Polyuria\")[\"class\"].value_counts().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Visualizing the distribution of Polydipsia\n",
    "polydipsia_plot = sns.countplot(x=dataFrame['Polydipsia'], hue=dataFrame['class'], palette='Greens')\n",
    "\n",
    "# Defining the criteria for the cross-tabulation\n",
    "cross_tab_criteria = ['Polydipsia', 'class']\n",
    "\n",
    "# Creating a color map\n",
    "color_map = sns.light_palette(\"blue\", as_cmap=True)\n",
    "\n",
    "# Creating a cross-tabulation and normalizing the values by column\n",
    "cross_tab = pd.crosstab(dataFrame[cross_tab_criteria[0]], dataFrame[cross_tab_criteria[1]], normalize='columns')\n",
    "\n",
    "# Converting the proportions to percentages and rounding to 2 decimal places\n",
    "cross_tab_percentages = cross_tab.mul(100).round(2)\n",
    "\n",
    "# Applying a gradient coloring to the table\n",
    "styled_cross_tab = cross_tab_percentages.style.background_gradient(cmap=color_map)\n",
    "\n",
    "styled_cross_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis based on Polydipsia\n",
    "dataFrame.groupby(\"Polydipsia\")[\"class\"].value_counts().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Visualizing the distribution of sudden weight loss\n",
    "sudden_weight_loss_plot = sns.countplot(x=dataFrame['sudden weight loss'], hue=dataFrame['class'], palette='Greens')\n",
    "\n",
    "# Defining the criteria for the cross-tabulation\n",
    "cross_tab_criteria = ['sudden weight loss', 'class']\n",
    "\n",
    "# Creating a color map\n",
    "color_map = sns.light_palette(\"blue\", as_cmap=True)\n",
    "\n",
    "# Creating a cross-tabulation and normalizing the values by column\n",
    "cross_tab = pd.crosstab(dataFrame[cross_tab_criteria[0]], dataFrame[cross_tab_criteria[1]], normalize='columns')\n",
    "\n",
    "# Converting the proportions to percentages and rounding to 2 decimal places\n",
    "cross_tab_percentages = cross_tab.mul(100).round(2)\n",
    "\n",
    "# Applying a gradient coloring to the table\n",
    "styled_cross_tab = cross_tab_percentages.style.background_gradient(cmap=color_map)\n",
    "\n",
    "styled_cross_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis based on sudden weight loss\n",
    "dataFrame.groupby(\"sudden weight loss\")[\"class\"].value_counts().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Visualizing the distribution of weakness\n",
    "weakness_plot = sns.countplot(x=dataFrame['weakness'], hue=dataFrame['class'], palette='Greens')\n",
    "\n",
    "# Defining the criteria for the cross-tabulation\n",
    "cross_tab_criteria = ['weakness', 'class']\n",
    "\n",
    "# Creating a color map\n",
    "color_map = sns.light_palette(\"blue\", as_cmap=True)\n",
    "\n",
    "# Creating a cross-tabulation and normalizing the values by column\n",
    "cross_tab = pd.crosstab(dataFrame[cross_tab_criteria[0]], dataFrame[cross_tab_criteria[1]], normalize='columns')\n",
    "\n",
    "# Converting the proportions to percentages and rounding to 2 decimal places\n",
    "cross_tab_percentages = cross_tab.mul(100).round(2)\n",
    "\n",
    "# Applying a gradient coloring to the table\n",
    "styled_cross_tab = cross_tab_percentages.style.background_gradient(cmap=color_map)\n",
    "\n",
    "styled_cross_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis based on  weakness\n",
    "dataFrame.groupby(\"weakness\")[\"class\"].value_counts().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Visualizing the distribution of Polyphagia\n",
    "polyphagia_plot = sns.countplot(x=dataFrame['Polyphagia'], hue=dataFrame['class'], palette='Greens')\n",
    "\n",
    "# Defining the criteria for the cross-tabulation\n",
    "cross_tab_criteria = ['Polyphagia', 'class']\n",
    "\n",
    "# Creating a color map\n",
    "color_map = sns.light_palette(\"blue\", as_cmap=True)\n",
    "\n",
    "# Creating a cross-tabulation and normalizing the values by column\n",
    "cross_tab = pd.crosstab(dataFrame[cross_tab_criteria[0]], dataFrame[cross_tab_criteria[1]], normalize='columns')\n",
    "\n",
    "# Converting the proportions to percentages and rounding to 2 decimal places\n",
    "cross_tab_percentages = cross_tab.mul(100).round(2)\n",
    "\n",
    "# Applying a gradient coloring to the table\n",
    "styled_cross_tab = cross_tab_percentages.style.background_gradient(cmap=color_map)\n",
    "\n",
    "styled_cross_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis based on Polyphagia\n",
    "dataFrame.groupby(\"Polyphagia\")[\"class\"].value_counts().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Visualizing the distribution of Genital thrush\n",
    "genital_thrush_plot = sns.countplot(x=dataFrame['Genital thrush'], hue=dataFrame['class'], palette='Greens')\n",
    "\n",
    "# Defining the criteria for the cross-tabulation\n",
    "cross_tab_criteria = ['Genital thrush', 'class']\n",
    "\n",
    "# Creating a color map\n",
    "color_map = sns.light_palette(\"blue\", as_cmap=True)\n",
    "\n",
    "# Creating a cross-tabulation and normalizing the values by column\n",
    "cross_tab = pd.crosstab(dataFrame[cross_tab_criteria[0]], dataFrame[cross_tab_criteria[1]], normalize='columns')\n",
    "\n",
    "# Converting the proportions to percentages and rounding to 2 decimal places\n",
    "cross_tab_percentages = cross_tab.mul(100).round(2)\n",
    "\n",
    "# Applying a gradient coloring to the table\n",
    "styled_cross_tab = cross_tab_percentages.style.background_gradient(cmap=color_map)\n",
    "\n",
    "styled_cross_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis based on Genital thrush\n",
    "dataFrame.groupby(\"Genital thrush\")[\"class\"].value_counts().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Visualizing the distribution of visual blurring\n",
    "visual_blurring_plot = sns.countplot(x=dataFrame['visual blurring'], hue=dataFrame['class'], palette='Greens')\n",
    "\n",
    "# Defining the criteria for the cross-tabulation\n",
    "cross_tab_criteria = ['visual blurring', 'class']\n",
    "\n",
    "# Creating a color map\n",
    "color_map = sns.light_palette(\"blue\", as_cmap=True)\n",
    "\n",
    "# Creating a cross-tabulation and normalizing the values by column\n",
    "cross_tab = pd.crosstab(dataFrame[cross_tab_criteria[0]], dataFrame[cross_tab_criteria[1]], normalize='columns')\n",
    "\n",
    "# Converting the proportions to percentages and rounding to 2 decimal places\n",
    "cross_tab_percentages = cross_tab.mul(100).round(2)\n",
    "\n",
    "# Applying a gradient coloring to the table\n",
    "styled_cross_tab = cross_tab_percentages.style.background_gradient(cmap=color_map)\n",
    "\n",
    "styled_cross_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis based on visual blurring\n",
    "dataFrame.groupby(\"visual blurring\")[\"class\"].value_counts().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Visualizing the distribution of Itching\n",
    "itching_plot = sns.countplot(x=dataFrame['Itching'], hue=dataFrame['class'], palette='Greens')\n",
    "\n",
    "# Defining the criteria for the cross-tabulation\n",
    "cross_tab_criteria = ['Itching', 'class']\n",
    "\n",
    "# Creating a color map\n",
    "color_map = sns.light_palette(\"blue\", as_cmap=True)\n",
    "\n",
    "# Creating a cross-tabulation and normalizing the values by column\n",
    "cross_tab = pd.crosstab(dataFrame[cross_tab_criteria[0]], dataFrame[cross_tab_criteria[1]], normalize='columns')\n",
    "\n",
    "# Converting the proportions to percentages and rounding to 2 decimal places\n",
    "cross_tab_percentages = cross_tab.mul(100).round(2)\n",
    "\n",
    "# Applying a gradient coloring to the table\n",
    "styled_cross_tab = cross_tab_percentages.style.background_gradient(cmap=color_map)\n",
    "\n",
    "styled_cross_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis based on Itching\n",
    "dataFrame.groupby(\"Itching\")[\"class\"].value_counts().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Visualizing the distribution of Irritability\n",
    "irritability_plot = sns.countplot(x=dataFrame['Irritability'], hue=dataFrame['class'], palette='Greens')\n",
    "\n",
    "# Defining the criteria for the cross-tabulation\n",
    "cross_tab_criteria = ['Irritability', 'class']\n",
    "\n",
    "# Creating a color map\n",
    "color_map = sns.light_palette(\"blue\", as_cmap=True)\n",
    "\n",
    "# Creating a cross-tabulation and normalizing the values by column\n",
    "cross_tab = pd.crosstab(dataFrame[cross_tab_criteria[0]], dataFrame[cross_tab_criteria[1]], normalize='columns')\n",
    "\n",
    "# Converting the proportions to percentages and rounding to 2 decimal places\n",
    "cross_tab_percentages = cross_tab.mul(100).round(2)\n",
    "\n",
    "# Applying a gradient coloring to the table\n",
    "styled_cross_tab = cross_tab_percentages.style.background_gradient(cmap=color_map)\n",
    "\n",
    "styled_cross_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis based on Irritability\n",
    "dataFrame.groupby(\"Irritability\")[\"class\"].value_counts().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Visualizing the distribution of delayed healing\n",
    "delayed_healing_plot = sns.countplot(x=dataFrame['delayed healing'], hue=dataFrame['class'], palette='Greens')\n",
    "\n",
    "# Defining the criteria for the cross-tabulation\n",
    "cross_tab_criteria = ['delayed healing', 'class']\n",
    "\n",
    "# Creating a color map\n",
    "color_map = sns.light_palette(\"blue\", as_cmap=True)\n",
    "\n",
    "# Creating a cross-tabulation and normalizing the values by column\n",
    "cross_tab = pd.crosstab(dataFrame[cross_tab_criteria[0]], dataFrame[cross_tab_criteria[1]], normalize='columns')\n",
    "\n",
    "# Converting the proportions to percentages and rounding to 2 decimal places\n",
    "cross_tab_percentages = cross_tab.mul(100).round(2)\n",
    "\n",
    "# Applying a gradient coloring to the table\n",
    "styled_cross_tab = cross_tab_percentages.style.background_gradient(cmap=color_map)\n",
    "\n",
    "styled_cross_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis based on delayed healing\n",
    "dataFrame.groupby(\"delayed healing\")[\"class\"].value_counts().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Visualizing the distribution of partial paresis\n",
    "partial_paresis_plot = sns.countplot(x=dataFrame['partial paresis'], hue=dataFrame['class'], palette='Greens')\n",
    "\n",
    "# Defining the criteria for the cross-tabulation\n",
    "cross_tab_criteria = ['partial paresis', 'class']\n",
    "\n",
    "# Creating a color map\n",
    "color_map = sns.light_palette(\"blue\", as_cmap=True)\n",
    "\n",
    "# Creating a cross-tabulation and normalizing the values by column\n",
    "cross_tab = pd.crosstab(dataFrame[cross_tab_criteria[0]], dataFrame[cross_tab_criteria[1]], normalize='columns')\n",
    "\n",
    "# Converting the proportions to percentages and rounding to 2 decimal places\n",
    "cross_tab_percentages = cross_tab.mul(100).round(2)\n",
    "\n",
    "# Applying a gradient coloring to the table\n",
    "styled_cross_tab = cross_tab_percentages.style.background_gradient(cmap=color_map)\n",
    "\n",
    "styled_cross_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis based on partial paresis\n",
    "dataFrame.groupby(\"partial paresis\")[\"class\"].value_counts().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Visualizing the distribution of muscle stiffness\n",
    "muscle_stiffness_plot = sns.countplot(x=dataFrame['muscle stiffness'], hue=dataFrame['class'], palette='Greens')\n",
    "\n",
    "# Defining the criteria for the cross-tabulation\n",
    "cross_tab_criteria = ['muscle stiffness', 'class']\n",
    "\n",
    "# Creating a color map\n",
    "color_map = sns.light_palette(\"blue\", as_cmap=True)\n",
    "\n",
    "# Creating a cross-tabulation and normalizing the values by column\n",
    "cross_tab = pd.crosstab(dataFrame[cross_tab_criteria[0]], dataFrame[cross_tab_criteria[1]], normalize='columns')\n",
    "\n",
    "# Converting the proportions to percentages and rounding to 2 decimal places\n",
    "cross_tab_percentages = cross_tab.mul(100).round(2)\n",
    "\n",
    "# Applying a gradient coloring to the table\n",
    "styled_cross_tab = cross_tab_percentages.style.background_gradient(cmap=color_map)\n",
    "\n",
    "styled_cross_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis based on muscle stiffness\n",
    "dataFrame.groupby(\"muscle stiffness\")[\"class\"].value_counts().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Visualizing the distribution of Alopecia\n",
    "alopecia_plot = sns.countplot(x=dataFrame['Alopecia'], hue=dataFrame['class'], palette='Greens')\n",
    "\n",
    "# Defining the criteria for the cross-tabulation\n",
    "cross_tab_criteria = ['Alopecia', 'class']\n",
    "\n",
    "# Creating a color map\n",
    "color_map = sns.light_palette(\"blue\", as_cmap=True)\n",
    "\n",
    "# Creating a cross-tabulation and normalizing the values by column\n",
    "cross_tab = pd.crosstab(dataFrame[cross_tab_criteria[0]], dataFrame[cross_tab_criteria[1]], normalize='columns')\n",
    "\n",
    "# Converting the proportions to percentages and rounding to 2 decimal places\n",
    "cross_tab_percentages = cross_tab.mul(100).round(2)\n",
    "\n",
    "# Applying a gradient coloring to the table\n",
    "styled_cross_tab = cross_tab_percentages.style.background_gradient(cmap=color_map)\n",
    "\n",
    "styled_cross_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis based on Alopecia\n",
    "dataFrame.groupby(\"Alopecia\")[\"class\"].value_counts().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Visualizing the distribution of Obesity\n",
    "obesity_plot = sns.countplot(x=dataFrame['Obesity'], hue=dataFrame['class'], palette='Greens')\n",
    "\n",
    "# Defining the criteria for the cross-tabulation\n",
    "cross_tab_criteria = ['Obesity', 'class']\n",
    "\n",
    "# Creating a color map\n",
    "color_map = sns.light_palette(\"blue\", as_cmap=True)\n",
    "\n",
    "# Creating a cross-tabulation and normalizing the values by column\n",
    "cross_tab = pd.crosstab(dataFrame[cross_tab_criteria[0]], dataFrame[cross_tab_criteria[1]], normalize='columns')\n",
    "\n",
    "# Converting the proportions to percentages and rounding to 2 decimal places\n",
    "cross_tab_percentages = cross_tab.mul(100).round(2)\n",
    "\n",
    "# Applying a gradient coloring to the table\n",
    "styled_cross_tab = cross_tab_percentages.style.background_gradient(cmap=color_map)\n",
    "\n",
    "styled_cross_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis based on Obesity\n",
    "dataFrame.groupby(\"Obesity\")[\"class\"].value_counts().unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occurences of Symptoms in patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to create a bar plot\n",
    "def plotBar(value, title):\n",
    "    # Set the figure size\n",
    "    plt.figure(figsize=(6,4))\n",
    "    # Get the counts of each unique value in the input series\n",
    "    value_counts = value.value_counts()\n",
    "    # Create a list of colors, 'blue' for 'Yes' and 'red' for other values\n",
    "    colors = ['green' if v == 'Yes' else 'red' for v in value_counts.index]\n",
    "    # Create a bar plot with the unique values as the x-axis and their counts as the y-axis\n",
    "    plt.bar(value_counts.index, value_counts.values, color=colors)\n",
    "    # Set the title of the plot\n",
    "    plt.title(title)\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# Create a new dataframe that includes all columns from the original dataframe except 'Age', 'class', and 'Gender'\n",
    "df_symptoms = dataFrame[dataFrame.columns.difference([\"Age\", \"class\", \"Gender\"])]\n",
    "\n",
    "# For each column in the new dataframe\n",
    "for column in df_symptoms.columns:\n",
    "    # Call the plotBar function with the column data and the capitalized column name as the title\n",
    "    plotBar(df_symptoms[column], column.capitalize())\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Filter the DataFrame based on the integer value in 'class' column\n",
    "positive_diabetes = dataFrame[dataFrame[\"class\"] == 1].copy()\n",
    "positive_diabetes.drop(['Age', 'Gender', 'class'], axis=1, inplace=True)\n",
    "\n",
    "# Map binary values to 'Yes' and 'No'\n",
    "positive_diabetes = positive_diabetes.replace({1: 'Yes', 0: 'No'})\n",
    "\n",
    "# Check and transform the data\n",
    "symptom_counts = positive_diabetes.apply(pd.Series.value_counts).transpose()\n",
    "\n",
    "# Ensure 'Yes' and 'No' columns exist\n",
    "if 'Yes' in symptom_counts.columns and 'No' in symptom_counts.columns:\n",
    "    symptom_counts['Symptom_Present_Percentage'] = symptom_counts['Yes'] / (symptom_counts['Yes'] + symptom_counts['No']) * 100\n",
    "    symptom_counts['Symptom_Absent_Percentage'] = symptom_counts['No'] / (symptom_counts['Yes'] + symptom_counts['No']) * 100\n",
    "    symptom_counts.drop(['Yes', 'No'], inplace=True, axis=1)\n",
    "\n",
    "    print('\\033[1m' + '\\n\\t People who were tested positive for Diabetes')\n",
    "    print('**********************************************************')\n",
    "    sorted_symptoms = symptom_counts.sort_values(by='Symptom_Present_Percentage', ascending=False)\n",
    "    print(sorted_symptoms)\n",
    "else:\n",
    "    print(\"Error: 'Yes' and/or 'No' columns not found in symptom_counts DataFrame\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing 'Positive' to 1 and 'Negative' to 0\n",
    "dataFrame['class'] = dataFrame['class'].replace({'Positive': 1, 'Negative': 0})\n",
    "dataFrame['class'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the DataFrame into features and target\n",
    "features = dataFrame.drop(['class'], axis=1)  # All columns except 'class' are considered as features\n",
    "target = dataFrame['class']  # 'class' column is the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and store columns in the features DataFrame that are of type 'object'\n",
    "object_columns = features.columns[features.dtypes == 'object']\n",
    "\n",
    "# Convert the identified columns to lowercase\n",
    "features.columns = features.columns.str.lower()\n",
    "\n",
    "#convert column to lowercase\n",
    "object_columns = object_columns.str.lower()\n",
    "\n",
    "print(object_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the information of the DataFrame\n",
    "print (features.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a LabelEncoder object\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Iterate over each feature in the object list\n",
    "for column in object_columns:\n",
    "    # Convert the column to string type and perform label encoding\n",
    "    features[column] = label_encoder.fit_transform(features[column].astype(str))\n",
    "\n",
    "# Print the information of the DataFrame\n",
    "print(features.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = features.corr()\n",
    "\n",
    "# Create a new figure for the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Create a heatmap\n",
    "heatmap = sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='viridis', ax=ax)\n",
    "\n",
    "# Set the title of the heatmap\n",
    "heatmap.set_title(\"Correlation Matrix of Diabetes Dataset\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select the 10 best features based on the chi-squared (chi^2) statistical test for non-negative features\n",
    "k_best_selector = SelectKBest(score_func=chi2, k=10)\n",
    "fitted_selector = k_best_selector.fit(features, target)\n",
    "\n",
    "# Create a DataFrame with the scores from the chi^2 test\n",
    "score_data = pd.DataFrame(fitted_selector.scores_)\n",
    "\n",
    "# Create a DataFrame with the column names\n",
    "column_data = pd.DataFrame(features.columns)\n",
    "\n",
    "# Concatenate the two DataFrames along the columns\n",
    "feature_scores = pd.concat([column_data, score_data], axis=1)\n",
    "feature_scores.columns = ['Feature', 'Score']\n",
    "feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the top 10 features based on chi-squared scores\n",
    "top_chi2_features = feature_scores.nlargest(10, 'Score')\n",
    "top_chi2_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of the top 10 features selected by the chi-squared test\n",
    "top_chi2_features = fitted_selector.get_support(indices=True)\n",
    "top_chi2_features = [column for column in features.columns[top_chi2_features]]\n",
    "top_chi2_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pie chart of the features\n",
    "featureview = pd.Series(fitted_selector.scores_, index=features.columns)\n",
    "featureview.plot(kind='pie', figsize=(10, 10))  # Increase the size as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features with a variance higher than 0.5 * (1 - 0.5)\n",
    "#variance_selector = VarianceThreshold(threshold=(0.5 * (1 - 0.5)))\n",
    "variance_selector = VarianceThreshold(threshold=0.24)\n",
    "fitted_variance = variance_selector.fit(features)\n",
    "print(fitted_variance)\n",
    "\n",
    "# Create a DataFrame with the variances\n",
    "variance_data = pd.DataFrame(fitted_variance.variances_)\n",
    "\n",
    "# Create a DataFrame with the column names\n",
    "column_data1 = pd.DataFrame(features.columns)\n",
    "\n",
    "# Concatenate the two DataFrames along the columns\n",
    "high_variance_features = pd.concat([variance_data, column_data1], axis=1)\n",
    "high_variance_features.columns = ['Variance', 'Feature']\n",
    "\n",
    "# Select the features with a variance higher than 0.2\n",
    "top_variance_features = high_variance_features[high_variance_features['Variance'] > 0.24]\n",
    "top_variance_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of the features selected by the variance threshold\n",
    "selected_features_indices = fitted_variance.get_support(indices=True)\n",
    "selected_features = [column for column in features.columns[selected_features_indices]]\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the selected features\n",
    "final_selected_features = list(set(top_chi2_features).union(set(selected_features)))\n",
    "print(final_selected_features)\n",
    "# Select these features from the original dataset\n",
    "X_final = features[final_selected_features]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#X_FS = X[['Polydipsia','sudden weight loss','partial paresis','Irritability','Polyphagia','Age','visual blurring']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into 80% training data and 20% testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, target, test_size = 0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax Scaling\n",
    "minmax = MinMaxScaler()\n",
    "X_train[['age']] = minmax.fit_transform(X_train[['age']])\n",
    "X_test[['age']] = minmax.transform(X_test[['age']])\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building/Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good for binary classification problems like diabetes (yes or no). It's interpretable and works well with smaller datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the Logistic Regression model\n",
    "log_reg_model = LogisticRegression(random_state=0, penalty='l2')\n",
    "\n",
    "# Train the model\n",
    "log_reg_model.fit(X_train, y_train)\n",
    "\n",
    "# Define the cross-validation strategy\n",
    "strat_k_fold = StratifiedKFold(n_splits=10)\n",
    "\n",
    "# Compute cross-validation scores\n",
    "cv_scores = cross_val_score(log_reg_model, X_train, y_train, cv=strat_k_fold, scoring='accuracy')\n",
    "\n",
    "# Predict the target for the test data\n",
    "y_predict_lr = log_reg_model.predict(X_test)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "acc_score = accuracy_score(y_test, y_predict_lr)\n",
    "prec_score = precision_score(y_test,y_predict_lr)\n",
    "rec_score = recall_score(y_test, y_predict_lr)\n",
    "f1_sc = f1_score(y_test, y_predict_lr)\n",
    "\n",
    "# Store the results in a DataFrame\n",
    "log_reg_results = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression'],\n",
    "    'Accuracy': [acc_score],\n",
    "    'Cross Val Accuracy': [cv_scores.mean()],\n",
    "    'Precision': [prec_score],\n",
    "    'Recall': [rec_score],\n",
    "    'F1 Score': [f1_sc]\n",
    "})\n",
    "\n",
    "# Display the results\n",
    "log_reg_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building/Random Forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the Random Forest model\n",
    "random_forest_model = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# Train the model\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "# Define the cross-validation strategy\n",
    "strat_k_fold = StratifiedKFold(n_splits=10, random_state=7, shuffle=True)\n",
    "\n",
    "# Compute cross-validation scores\n",
    "cv_scores_rf = cross_val_score(random_forest_model, X_train, y_train, cv=strat_k_fold, scoring='accuracy')\n",
    "\n",
    "# Predict the target for the test data\n",
    "y_predict_rf = random_forest_model.predict(X_test)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "acc_score_rf = accuracy_score(y_test, y_predict_rf)\n",
    "prec_score_rf = precision_score(y_test, y_predict_rf)\n",
    "rec_score_rf = recall_score(y_test, y_predict_rf)\n",
    "f1_sc_rf = f1_score(y_test, y_predict_rf)\n",
    "\n",
    "# Store the results in a DataFrame\n",
    "random_forest_results = pd.DataFrame({\n",
    "    'Model': ['Random Forest'],\n",
    "    'Accuracy': [acc_score_rf],\n",
    "    'Cross Val Accuracy': [cv_scores_rf.mean()],\n",
    "    'Precision': [prec_score_rf],\n",
    "    'Recall': [rec_score_rf],\n",
    "    'F1 Score': [f1_sc_rf]\n",
    "})\n",
    "\n",
    "# Display the results\n",
    "random_forest_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building/ Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Neural Network Model\n",
    "def create_model(input_dim):\n",
    "    # Define the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=input_dim, activation='relu'))  # Hidden layer\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Output layer\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the neural network model\n",
    "nn_model = create_model(X_train.shape[1])\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the neural network\n",
    "nn_history = nn_model.fit(X_train, y_train, epochs=100, batch_size=10, \n",
    "                          verbose=1, callbacks=[early_stopping], validation_split=0.1)\n",
    "\n",
    "# Predict the target for the test data\n",
    "y_predict_nn = (nn_model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "nn_acc_score = accuracy_score(y_test, y_predict_nn)\n",
    "nn_prec_score = precision_score(y_test,y_predict_nn)\n",
    "nn_rec_score = recall_score(y_test, y_predict_nn)\n",
    "nn_f1_sc = f1_score(y_test, y_predict_nn)\n",
    "\n",
    "# Store the results in a DataFrame\n",
    "result_nn = pd.DataFrame([['Neural Network', nn_acc_score, nn_prec_score,nn_rec_score, nn_f1_sc]], columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\n",
    "\n",
    "# Display the results\n",
    "result_nn \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building/ Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the SVM model\n",
    "svm_classifier = svm.SVC(kernel='linear', random_state=0)\n",
    "\n",
    "# Train the model\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Define the cross-validation strategy\n",
    "strat_k_fold = StratifiedKFold(n_splits=10)\n",
    "\n",
    "# Compute cross-validation scores\n",
    "cv_scores_svm = cross_val_score(svm_classifier, X_train, y_train, cv=strat_k_fold, scoring='accuracy')\n",
    "\n",
    "# Predict the target for the test data\n",
    "y_pred_svm = svm_classifier.predict(X_test)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "acc_score_svm = accuracy_score(y_test, y_pred_svm)\n",
    "prec_score_svm = precision_score(y_test, y_pred_svm)\n",
    "rec_score_svm = recall_score(y_test, y_pred_svm)\n",
    "f1_sc_svm = f1_score(y_test, y_pred_svm)\n",
    "\n",
    "# Store the results in a DataFrame\n",
    "results_svm = pd.DataFrame({\n",
    "    'Model': ['SVM'],\n",
    "    'Accuracy': [acc_score_svm],\n",
    "    'Cross Val Accuracy': [cv_scores_svm.mean()],\n",
    "    'Precision': [prec_score_svm],\n",
    "    'Recall': [rec_score_svm],\n",
    "    'F1 Score': [f1_sc_svm]\n",
    "})\n",
    "\n",
    "# Display the results\n",
    "results_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building/ K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the KNN model\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "\n",
    "# Train the model\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Define the cross-validation strategy\n",
    "strat_k_fold = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
    "\n",
    "# Compute cross-validation scores\n",
    "cv_scores_knn = cross_val_score(knn_classifier, X_train, y_train, cv=strat_k_fold, scoring='accuracy')\n",
    "\n",
    "# Predict the target for the test data\n",
    "y_predict_knn = knn_classifier.predict(X_test)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "acc_score_knn = accuracy_score(y_test, y_predict_knn)\n",
    "prec_score_knn = precision_score(y_test,y_predict_knn)\n",
    "rec_score_knn = recall_score(y_test, y_predict_knn)\n",
    "f1_sc_knn = f1_score(y_test,y_predict_knn)\n",
    "\n",
    "# Store the results in a DataFrame\n",
    "knn_results = pd.DataFrame({\n",
    "    'Model': ['KNN'],\n",
    "    'Accuracy': [acc_score_knn],\n",
    "    'Cross Val Accuracy': [cv_scores_knn.mean()],\n",
    "    'Precision': [prec_score_knn],\n",
    "    'Recall': [rec_score_knn],\n",
    "    'F1 Score': [f1_sc_knn]\n",
    "})\n",
    "\n",
    "# Display the results\n",
    "knn_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building/XGB Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize XGBoost classifier\n",
    "classifier = xgb.XGBClassifier(random_state=0)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_predict_svm = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "acc = accuracy_score(y_test,y_predict_svm)\n",
    "prec = precision_score(y_test,y_predict_svm)\n",
    "rec = recall_score(y_test,y_predict_svm)\n",
    "f1 = f1_score(y_test,y_predict_svm)\n",
    "\n",
    "# Perform cross-validation\n",
    "kfold = KFold(n_splits=10, random_state=0, shuffle=True)\n",
    "acc_xgb = cross_val_score(classifier, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "\n",
    "\n",
    "# Compile the results into a DataFrame\n",
    "results_xgb = pd.DataFrame([['XGB', acc, acc_xgb.mean(), prec, rec, f1]], \n",
    "                       columns=['Model', 'Accuracy', 'Cross Val Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\n",
    "# Display the results\n",
    "results_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the results of all models\n",
    "combined_results = pd.concat([log_reg_results, results_svm, knn_results,result_nn,random_forest_results], ignore_index=True)\n",
    "\n",
    "# Display the combined results\n",
    "combined_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix to visualize the performance of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming y_test is your actual test labels and y_predict_* are the predicted labels from each model\n",
    "y_predict_lr = log_reg_model.predict(X_test) # Logistic Regression predictions\n",
    "y_predict_svm = svm_classifier.predict(X_test) # SVM predictions\n",
    "y_predict_rf = random_forest_model.predict(X_test) # Random Forest predictions\n",
    "y_predict_knn = knn_classifier.predict(X_test) # Random Forest predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check the unique values in the predictions and the target variable\n",
    "print(\"Unique values in y_test:\", pd.unique(y_test))\n",
    "print(\"Unique values in y_predict_lr:\", pd.unique(y_predict_lr))\n",
    "print(\"Unique values in y_predict_svm:\", pd.unique(y_predict_svm))\n",
    "print(\"Unique values in y_predict_rf:\", pd.unique(y_predict_rf))\n",
    "print(\"Unique values in y_predict_knn:\", pd.unique(y_predict_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create confusion matrices for each model\n",
    "cm_lr = confusion_matrix(y_test, y_predict_lr)\n",
    "cm_svm = confusion_matrix(y_test, y_predict_svm)\n",
    "cm_rf = confusion_matrix(y_test, y_predict_rf)\n",
    "cm_knn = confusion_matrix(y_test, y_predict_knn)\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(cm, model_name):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix for {model_name}')\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Plotting confusion matrices\n",
    "plot_confusion_matrix(cm_lr, \"Logistic Regression\")\n",
    "plot_confusion_matrix(cm_svm, \"SVM\")\n",
    "plot_confusion_matrix(cm_rf, \"Random Forest\")\n",
    "plot_confusion_matrix(cm_knn, \"KNN\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Generate the confusion matrix for the neural network model\n",
    "nn_confusion_matrix = confusion_matrix(y_test,y_predict_nn)\n",
    "\n",
    "# Create a heatmap for the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(nn_confusion_matrix, annot=True, fmt=\"d\", cmap='Blues', \n",
    "            xticklabels=['Predicted Negative', 'Predicted Positive'], \n",
    "            yticklabels=['Actual Negative', 'Actual Positive'])\n",
    "plt.title('Confusion Matrix for Neural Network Model')\n",
    "plt.ylabel('True Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix is a tool often used in classification tasks to visualise the performance of an algorithm. Typically, it is a square matrix that compares the actual target values with the values predicted by the model. To provide a detailed explanation and interpretation, I will describe the common components of a confusion matrix:\n",
    "\n",
    "True Positives (TP): These are cases where the model correctly predicts the positive class.\n",
    "\n",
    "True Negatives (TN): These are cases where the model correctly predicts the negative class.\n",
    "\n",
    "False Positives (FP), also known as Type I error: These are cases where the model incorrectly predicts the positive class.\n",
    "\n",
    "False Negatives (FN), also known as Type II error: These are cases where the model incorrectly predicts the negative class.\n",
    "\n",
    "The confusion matrix typically looks like this:\n",
    "\n",
    "Actual \\ Predicted\tPositive Prediction\tNegative Prediction\n",
    "Actual Positive\t              TP\t         FN\n",
    "Actual Negative\t              FP\t         TN\n",
    "\n",
    "\n",
    "Interpretation:\n",
    "Accuracy: (TP + TN) / (TP + TN + FP + FN). This measures how often the classifier makes the correct prediction. It’s the ratio of the number of correct predictions to the total number of predictions.\n",
    "\n",
    "Precision: TP / (TP + FP). This measures how many of the items identified as positive were actually positive. It’s important in contexts where False Positives are more significant than False Negatives.\n",
    "\n",
    "Recall (Sensitivity): TP / (TP + FN). This measures how many of the actual positive items were identified correctly. It’s important in contexts where False Negatives are more significant than False Positives.\n",
    "\n",
    "F1 Score: 2 * (Precision * Recall) / (Precision + Recall). This is a harmonic mean of precision and recall and gives a balance between them.\n",
    "\n",
    "Specificity: TN / (TN + FP). This measures the proportion of actual negatives that are correctly identified as such.\n",
    "\n",
    "Contextual Importance:\n",
    "In medical testing, high recall might be more important because missing a positive (disease) case (False Negative) could be more detrimental than a False Positive.\n",
    "In spam detection, high precision might be more crucial because categorizing a legitimate email as spam (False Positive) is often seen as worse than not catching a spam email (False Negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class TestAllModels(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUp(cls):\n",
    "\n",
    "        # Load your test dataset\n",
    "        cls.dataset = pd.read_csv('../../AIBetic2Dataset/test_model_dataset.csv')\n",
    "\n",
    "         # Convert categorical columns to numerical\n",
    "        columns_to_convert = ['delayed healing', 'Polyuria', 'Alopecia', 'Gender', 'Itching', 'visual blurring', 'weakness', 'partial paresis', 'Polydipsia', 'Age', 'Polyphagia', 'sudden weight loss', 'Irritability']\n",
    "        for column in columns_to_convert:\n",
    "            cls.dataset[column] = cls.dataset[column].replace({'Yes': 1, 'No': 0})\n",
    "\n",
    "        cls.dataset['Gender'] = cls.dataset['Gender'].replace({'Male': 1, 'Female': 0})\n",
    "\n",
    "        cls.dataset['class'] = cls.dataset['class'].replace({'Positive': 1, 'Negative': 0})\n",
    "\n",
    "        # Convert column names to lowercase\n",
    "        cls.dataset.columns = map(str.lower, cls.dataset.columns)\n",
    "\n",
    "        # Select columns\n",
    "        selected_columns = ['itching', 'sudden weight loss', 'weakness', 'partial paresis', 'polyphagia', 'visual blurring', 'irritability', 'alopecia', 'polydipsia', 'delayed healing', 'polyuria', 'age', 'gender', 'class']\n",
    "\n",
    "\n",
    "        cls.dataset = cls.dataset[selected_columns]\n",
    "\n",
    "        # Split the dataset into features and target\n",
    "        cls.X_test = cls.dataset.drop('class', axis=1)\n",
    "        cls.y_test = cls.dataset['class']\n",
    "\n",
    "        # Scale the 'Age' column\n",
    "        minmax = MinMaxScaler()\n",
    "        cls.dataset[['age']] = minmax.fit_transform(cls.dataset[['age']])\n",
    "\n",
    "        # print(cls.dataset.head())\n",
    "\n",
    "        \n",
    "    def test_svm(self):\n",
    "        y_pred = svm_classifier.predict(self.X_test)\n",
    "        print(\"Model: Support Vector Machine\")\n",
    "        print(pd.DataFrame({'Actual': self.y_test, 'Predicted': y_pred}))\n",
    "\n",
    "    def test_log_reg(self):\n",
    "        y_pred = log_reg_model.predict(self.X_test)\n",
    "        print(\"Model: Logistic Regression\")\n",
    "        print(pd.DataFrame({'Actual': self.y_test, 'Predicted': y_pred}))\n",
    "\n",
    "    def test_nn(self):\n",
    "        y_pred = nn_model.predict(self.X_test)\n",
    "        y_pred = (y_pred > 0.5).astype(int)\n",
    "        y_pred = y_pred.flatten()\n",
    "        print(\"Model: Neural Network\")\n",
    "        print(pd.DataFrame({'Actual': self.y_test, 'Predicted': y_pred}))\n",
    "\n",
    "    def test_knn(self):\n",
    "        y_pred = knn_classifier.predict(self.X_test)\n",
    "        print(\"Model: K-Nearest Neighbors\")\n",
    "        print(pd.DataFrame({'Actual': self.y_test, 'Predicted': y_pred}))\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
